<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Edoardo Avenia">

<title>Introduzione all’Intelligenza Artificiale e agli LLM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="index_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap-98cad52ce7327bfd42cee12d434ba08f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


<link rel="stylesheet" href="../../style.css">
</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#hai-usato-lintelligenza-artificiale-nelle-ultime-24-ore" id="toc-hai-usato-lintelligenza-artificiale-nelle-ultime-24-ore" class="nav-link active" data-scroll-target="#hai-usato-lintelligenza-artificiale-nelle-ultime-24-ore">Hai Usato l’Intelligenza Artificiale nelle Ultime 24 Ore?</a></li>
  <li><a href="#cosè-davvero-lintelligenza-artificiale" id="toc-cosè-davvero-lintelligenza-artificiale" class="nav-link" data-scroll-target="#cosè-davvero-lintelligenza-artificiale">Cos’è Davvero l’Intelligenza Artificiale?</a></li>
  <li><a href="#lispirazione-dal-cervello-le-reti-neurali" id="toc-lispirazione-dal-cervello-le-reti-neurali" class="nav-link" data-scroll-target="#lispirazione-dal-cervello-le-reti-neurali">L’Ispirazione dal Cervello: Le Reti Neurali</a></li>
  <li><a href="#dal-linguaggio-ai-numeri-il-mondo-segreto-dei-token" id="toc-dal-linguaggio-ai-numeri-il-mondo-segreto-dei-token" class="nav-link" data-scroll-target="#dal-linguaggio-ai-numeri-il-mondo-segreto-dei-token">Dal Linguaggio ai Numeri: Il Mondo Segreto dei Token</a></li>
  <li><a href="#la-rivoluzione-transformer-quando-lattenzione-è-tutto" id="toc-la-rivoluzione-transformer-quando-lattenzione-è-tutto" class="nav-link" data-scroll-target="#la-rivoluzione-transformer-quando-lattenzione-è-tutto">La Rivoluzione Transformer: Quando l’Attenzione è Tutto</a></li>
  <li><a href="#larte-delladdestramento-come-nasce-unintelligenza-artificiale" id="toc-larte-delladdestramento-come-nasce-unintelligenza-artificiale" class="nav-link" data-scroll-target="#larte-delladdestramento-come-nasce-unintelligenza-artificiale">L’Arte dell’Addestramento: Come Nasce un’Intelligenza Artificiale</a></li>
  <li><a href="#il-prompt-e-le-proprietà-emergenti" id="toc-il-prompt-e-le-proprietà-emergenti" class="nav-link" data-scroll-target="#il-prompt-e-le-proprietà-emergenti">Il Prompt e le Proprietà Emergenti</a></li>
  <li><a href="#il-30-novembre-2022-il-giorno-in-cui-gli-llm-hanno-incontrato-il-mondo" id="toc-il-30-novembre-2022-il-giorno-in-cui-gli-llm-hanno-incontrato-il-mondo" class="nav-link" data-scroll-target="#il-30-novembre-2022-il-giorno-in-cui-gli-llm-hanno-incontrato-il-mondo">Il 30 Novembre 2022: Il Giorno in cui gli LLM hanno incontrato il Mondo</a></li>
  <li><a href="#come-funziona-davvero-un-llm-la-verità-dietro-la-magia" id="toc-come-funziona-davvero-un-llm-la-verità-dietro-la-magia" class="nav-link" data-scroll-target="#come-funziona-davvero-un-llm-la-verità-dietro-la-magia">Come Funziona Davvero un LLM: La Verità Dietro la Magia</a></li>
  <li><a href="#una-distinzione-cruciale" id="toc-una-distinzione-cruciale" class="nav-link" data-scroll-target="#una-distinzione-cruciale">Una Distinzione Cruciale</a></li>
  <li><a href="#glossario-essenziale" id="toc-glossario-essenziale" class="nav-link" data-scroll-target="#glossario-essenziale">Glossario Essenziale</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">
<div class="section-2">
<a href="/">← Home</a> | <a href="/corso-llm/">← Indice Corso</a>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduzione all’Intelligenza Artificiale e agli LLM</h1>
<p class="subtitle lead">Corso LLM - Modulo 1</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Autore/Autrice</div>
    <div class="quarto-title-meta-contents">
             <p>Edoardo Avenia </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="hai-usato-lintelligenza-artificiale-nelle-ultime-24-ore" class="level2">
<h2 class="anchored" data-anchor-id="hai-usato-lintelligenza-artificiale-nelle-ultime-24-ore">Hai Usato l’Intelligenza Artificiale nelle Ultime 24 Ore?</h2>
<p>Prima di iniziare, ti devo fare una domanda: hai usato l’intelligenza artificiale nelle ultime 24 ore?</p>
<p>Se stai pensando di no, probabilmente è perché quando senti “intelligenza artificiale” la tua mente va subito a ChatGPT o magari a quei robot futuristici dei film di fantascienza.</p>
<p>In realtà l’AI permea già ogni aspetto della nostra vita digitale, operando silenziosamente nell’ombra.</p>
<p>Stamattina, quando hai controllato le email, un algoritmo di machine learning ha filtrato lo spam per te. Se hai usato Google Maps, un’intelligenza artificiale ha analizzato i pattern di traffico in tempo reale per suggerirti il percorso migliore. Netflix ti ha suggerito quella serie che “stranamente” era proprio nel tuo stile? AI.</p>
<p>La correzione automatica che ha sistemato quel messaggio WhatsApp scritto di fretta? Ancora AI.</p>
<p>Instagram che ti mostra proprio i contenuti che ti interessano? Indovina un po’…</p>
<p>L’intelligenza artificiale non è il futuro. È il presente, ed è ovunque. Ma allora perché ChatGPT ha creato tanto scalpore?</p>
<p>In questo corso non tratteremo tutto il mondo AI ma ci concentreremo specificamente sugli LLM (Large Language Models), i modelli che hanno rivoluzionato il modo in cui interagiamo con i computer. Partiamo quindi con una domanda fondamentale.</p>
</section>
<section id="cosè-davvero-lintelligenza-artificiale" class="level2">
<h2 class="anchored" data-anchor-id="cosè-davvero-lintelligenza-artificiale">Cos’è Davvero l’Intelligenza Artificiale?</h2>
<p>L’intelligenza artificiale, nella sua essenza più pura, è qualunque sistema che cerca di risolvere problemi che normalmente richiederebbero l’intelletto umano. Non è magia, è matematica.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="immagini/ai_ml_dl_venn_diagram.png" class="img-fluid figure-img" style="width:57.0%"></p>
<figcaption>AI, Machine Learning e Deep Learning</figcaption>
</figure>
</div>
<p>L’insieme più esterno rappresenta l’Intelligenza Artificiale nel suo complesso. È un territorio vasto che include anche i sistemi esperti degli anni ’80 - quei programmi dove esperti umani codificavano manualmente ogni singola regola. Un sacco di regole del tipo “se questo allora quello”, migliaia e migliaia di condizioni per cercare di catturare la conoscenza umana.</p>
<p>All’interno di questo grande insieme incontriamo il Machine Learning. Qui avviene la prima rivoluzione concettuale. Invece di dire al computer esattamente cosa fare in ogni situazione, gli permettiamo di imparare dai dati. È un cambio di paradigma fondamentale, come passare dall’insegnare a qualcuno a seguire una ricetta passo passo al mostrargli migliaia di piatti e lasciare che capisca da solo come cucinarli.</p>
<p>Il Machine Learning opera in due fasi distinte. Prima c’è l’allenamento, dove il sistema apprende dai dati, poi l’utilizzo, dove il modello addestrato viene applicato a situazioni nuove. Ma come fa un computer a “imparare”? In fondo, le macchine capiscono solo numeri. Ed è proprio qui che sta il trucco: tutto - immagini, suoni, testi - viene convertito in numeri. Il Machine Learning diventa quindi l’arte di trovare pattern significativi in oceani di numeri.</p>
<p>Prendiamo l’esempio del filtro antispam. L’approccio Machine Learning è radicalmente diverso: mostriamo al computer montagne di email, spam e legittime in quantità massicce. Il sistema trova da solo i pattern che le distinguono, pattern così sottili e complessi che noi umani faticheremmo a descriverli a parole.</p>
<p>Una tecnica di ML molto usata sono le reti neurali. Qui la storia si fa affascinante, perché gli scienziati hanno guardato al cervello umano per trovare ispirazione.</p>
<p>Nel nostro cervello, miliardi di neuroni comunicano tra loro attraverso impulsi elettrici. Ogni neurone riceve segnali, li elabora, e decide se “attivarsi” per passare il segnale ad altri neuroni. È un sistema di una complessità vertiginosa, frutto di milioni di anni di evoluzione.</p>
<p>Video 3Blue1Brown su Reti Neurali: <a href="https://www.youtube.com/watch?v=aircAruvnKk">link al video</a></p>
<p>Le reti neurali artificiali tentano di catturare l’essenza di questo processo, ma con una semplificazione estrema. Un neurone artificiale è solo una funzione matematica che prende numeri in input, li moltiplica per dei pesi - che chiamiamo parametri - e produce un numero in output.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcbcBXfWLNyQdo2P1koN1BWh2_3KI4JdELZCJPGLa8VwQwES3h3NVeGifMpFi1G9GuSNCghhJp-mYiap21V9aSep536KD8cPUb-xHqswiERjhtgeM8w1bLAJbjYzFbbB19_RxPQGg?key=dhMVT6hYyFZOsGrf2guXGw.png" class="img-fluid figure-img" style="width:85.0%"></p>
<figcaption>Struttura di una rete neurale</figcaption>
</figure>
</div>
<p>fonte: <a href="https://www.nature.com/articles/s41377-024-01590-3">Nature - Photonic neural networks</a></p>
</section>
<section id="lispirazione-dal-cervello-le-reti-neurali" class="level2">
<h2 class="anchored" data-anchor-id="lispirazione-dal-cervello-le-reti-neurali">L’Ispirazione dal Cervello: Le Reti Neurali</h2>
<p>Andando ancora più in profondità nei nostri insiemi concentrici, dentro il Machine Learning troviamo il Deep Learning, e al suo cuore pulsano le reti neurali profonde.</p>
<p>Il “deep” in Deep Learning si riferisce alla profondità: invece di avere uno o due strati di neuroni, ne abbiamo decine o centinaia. E qui succede qualcosa di quasi magico. Ogni strato impara a riconoscere concetti sempre più astratti. I primi strati potrebbero riconoscere bordi e linee, i successivi forme semplici, poi parti di oggetti, fino ad arrivare agli strati finali che riconoscono concetti complessi e astratti.</p>
</section>
<section id="dal-linguaggio-ai-numeri-il-mondo-segreto-dei-token" class="level2">
<h2 class="anchored" data-anchor-id="dal-linguaggio-ai-numeri-il-mondo-segreto-dei-token">Dal Linguaggio ai Numeri: Il Mondo Segreto dei Token</h2>
<p>Ma arriviamo al cuore del problema quando parliamo di linguaggio. I computer lavorano con numeri, il linguaggio umano è fatto di parole, significati, sfumature. Come colmare questo abisso?</p>
<p>La risposta sta in due concetti fondamentali: token ed embedding. Un token è un “pezzo” di testo - può essere una parola intera, parte di una parola, o anche un singolo carattere - a cui viene assegnato un numero identificativo. “hello” è il token numero 24912, mentre “frutta” è spezzato in “fr” (token 1739) e “utta” (token 29215).</p>
<p>Questo è il tokenizzatore o200k_base usato da ChatGPT.</p>
<p>Ora facciamo una gara: trova la parola rappresentata con un singolo token più lunga possibile! Non valgono liste di spazi o punti, ma valgono tutte le parole. Prova a cercare parole lunghe in italiano e in inglese e confronta i risultati.</p>
<p><a href="https://platform.openai.com/tokenizer">Openai - Tokenizer</a></p>
<p>Lo scopo di questo esercizio è accorgersi che i token in inglese sono più efficienti perché gli algoritmi hanno molti più dati in inglese che in italiano. Infatti questo non è solo assegnare un ID ad ogni parola ma è trovare un modo efficiente di rappresentare il testo - una vera e propria forma di compressione linguistica.</p>
<p>Ma assegnare un semplice numero ID a ogni token non cattura il significato. Qui entrano in gioco gli embedding, rappresentazioni numeriche complesse che sono al cuore della “comprensione” linguistica dei modelli. Un embedding non è un singolo numero, ma un vettore di centinaia di dimensioni.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXePfhkbM3MZH8QTRCjdP9h9uy_mGJ0XxMGHPYGFRTOhjScvFykNUYV79sf0H6q-RmSmo5b1kYA2ovO4uepV5rSgysD14U2OmG2vSnZv2qkq0b3LavcPaDofQqNxyVZKL2YeWpbOEw?key=dhMVT6hYyFZOsGrf2guXGw.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Vettore 2D</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcog_3OLwFMUXNA5Uk6el1epjRI5exjba8_8JczNc12NKlncbxAxkqtACpMCqK3lhBvBwZke8xpBq-OLsQkHxSPyLcZtm6-a8zpYWKaO2rZ0IBDjXeX8bKIKPat_gT3ZKP2yRPexQ?key=dhMVT6hYyFZOsGrf2guXGw.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Vettore 3D</figcaption>
</figure>
</div>
<p>fonte: <a href="https://www.3blue1brown.com/lessons/vectors">3Blue1Brown - Vectors</a></p>
<p>Nel mondo degli LLM, gli embedding vivono in spazi vettoriali a migliaia di dimensioni, spesso tra 1024 e oltre 12.000, a seconda del modello e dell’uso.</p>
<p>La magia degli embedding sta nel fatto che parole con significati simili hanno embedding simili. Ma c’è di più: gli embedding cambiano in base al contesto. La parola “pesca” avrà un embedding diverso nella frase “Nel pancake adoro la pesca” rispetto a “Nel weekend adoro la pesca”. Il modello impara queste sottigliezze attraverso l’esposizione a miliardi di esempi durante l’addestramento, in questo modo la sua risposta peserà in modo diverso la parola pesca in base alle parole pancake e weekend.</p>
<p>Approfondimento: <a href="https://muneebsa.medium.com/deep-learning-101-lesson-30-understanding-text-with-attention-heatmaps-efe968a51bc2">Understanding Text with Attention Heatmaps</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfcG-m0n_mG8w4vZNst65aA4IAinKHyg1HABgbRFtxeXUEWsGGzJ3YiA9KM1eRivvGOtoRzUP4x9xhUjkzmOfgrezAUaNfvBvpvhltUC-ds0MemlAO6au7a1Cb8G3LWcFUSebSPkQ?key=dhMVT6hYyFZOsGrf2guXGw.png" class="img-fluid figure-img" style="width:85.0%"></p>
<figcaption>Attention Heatmap</figcaption>
</figure>
</div>
<p>Questi embedding sono così complessi e multidimensionali che nemmeno gli scienziati che li hanno creati li comprendono appieno. È parte del mistero della “black box” dell’AI: sappiamo che funziona, vediamo i risultati, ma i meccanismi interni rimangono in parte oscuri.</p>
</section>
<section id="la-rivoluzione-transformer-quando-lattenzione-è-tutto" class="level2">
<h2 class="anchored" data-anchor-id="la-rivoluzione-transformer-quando-lattenzione-è-tutto">La Rivoluzione Transformer: Quando l’Attenzione è Tutto</h2>
<p>Nel 2017, otto ricercatori di Google pubblicarono un paper dal titolo provocatorio: “Attention Is All You Need” - l’attenzione è tutto ciò che serve. Non potevano immaginare quanto profetico sarebbe stato quel titolo.</p>
<p>Prima dei Transformer, i modelli di linguaggio processavano il testo come se leggessero con una lente d’ingrandimento che mostra solo una parola alla volta, era un processo sequenziale, lento e limitato.</p>
<p>L’architettura Transformer rivoluzionò tutto introducendo il meccanismo di “attention”. Invece di processare sequenzialmente, il modello può “prestare attenzione” a tutte le parole contemporaneamente, capendo istantaneamente quali sono più rilevanti per il contesto. È come passare dal leggere una parola alla volta al poter abbracciare con lo sguardo l’intera pagina, focalizzandosi sui passaggi chiave.</p>
<ul>
<li>Attention Is All You Need: <a href="https://arxiv.org/abs/1706.03762">paper originale</a></li>
<li>The Illustrated Transformer: <a href="https://jalammar.github.io/illustrated-transformer/">guida visuale</a></li>
</ul>
</section>
<section id="larte-delladdestramento-come-nasce-unintelligenza-artificiale" class="level2">
<h2 class="anchored" data-anchor-id="larte-delladdestramento-come-nasce-unintelligenza-artificiale">L’Arte dell’Addestramento: Come Nasce un’Intelligenza Artificiale</h2>
<p>Creare un Large Language Model è un processo che ricorda più l’educazione di una mente che la programmazione di un computer. Si svolge in tre fasi distinte, ognuna cruciale per il risultato finale.</p>
<p>La prima fase è il pre-training, dove il modello viene esposto a quantità di testo che sfidano l’immaginazione. Parliamo di sostanzialmente tutto ciò che è stato scritto su internet: libri, articoli, pagine web, forum, codice di programmazione. Il compito del modello in questa fase è apparentemente semplice: prevedere la parola successiva. Data la sequenza “Il gatto è sul”, deve calcolare che “tetto” è più probabile di “mangiare”.</p>
<p>Un po’ come negli esercizi per imparare nuove lingue…</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="immagini/next_token_prediction_example.png" class="img-fluid figure-img" style="width:85.0%"></p>
<figcaption>Esercizio di completamento</figcaption>
</figure>
</div>
<p>Questo task di “next token prediction” sembra banale, ma da esso emergono capacità straordinarie. È come se imparando a prevedere la parola successiva in miliardi di contesti diversi, il modello sviluppasse una comprensione implicita della grammatica, della logica, dei fatti del mondo, persino dello stile e del tono.</p>
<p>La seconda fase è il supervised fine-tuning, dove il modello viene specializzato con esempi di conversazioni create da umani. Qui impara non solo a completare testi, ma a rispondere a domande, seguire istruzioni, mantenere una conversazione coerente.</p>
<p>La terza fase, l’alignment o allineamento, è forse la più delicata. L’allineamento è un processo ampio che mira a rendere il modello utile, sicuro e allineato con i valori umani. Durante questa fase si utilizzano diverse tecniche, tra cui il Reinforcement Learning from Human Feedback (RLHF). Con RLHF, il modello impara non solo a dare risposte corrette, ma risposte che gli umani preferiscono, attraverso un processo iterativo di feedback.</p>
</section>
<section id="il-prompt-e-le-proprietà-emergenti" class="level2">
<h2 class="anchored" data-anchor-id="il-prompt-e-le-proprietà-emergenti">Il Prompt e le Proprietà Emergenti</h2>
<p>Un momento importantissimo nella storia degli LLM è stato il 2020 con la pubblicazione del paper “Language Models are Few-Shot Learners” su GPT-3. Questo paper rivoluzionario, firmato da ricercatori del calibro di Ilya Sutskever e Dario Amodei, ha introdotto un concetto fondamentale: gli LLM non sono come gli altri algoritmi di Deep Learning, in particolare quelli precedenti per il Natural Language Processing.</p>
<p>Paper GPT-3: <a href="https://arxiv.org/pdf/2005.14165">Language Models are Few-Shot Learners</a></p>
<p>I modelli NLP tradizionali dovevano essere addestrati su un task specifico e potevano fare solo quello. Per esempio, un modello addestrato per tradurre non poteva rispondere a domande. Gli LLM invece dimostrano la capacità emergente di riuscire a fare task che non hanno mai visto esplicitamente durante l’addestramento, grazie a pochissimi esempi di quel task nell’input e grazie a prompt in linguaggio naturale. Una totale rivoluzione che ha cambiato per sempre il mondo del NLP.</p>
<p>Le proprietà emergenti dimostrano che, quando queste architetture vengono scalate a miliardi di parametri, diventano molto più che semplici ‘pappagalli stocastici’ o ‘T9 sotto steroidi’</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="immagini/llm_t9_with_steroids.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>T9 sotto steroidi</figcaption>
</figure>
</div>
<p>Il prompt è il testo che dai come input al modello, e lui prevederà la risposta corretta token per token. Nel pre-training il modello capisce il linguaggio, nel fine-tuning impara anche a seguire le istruzioni e mantenere lo stile chat. La combinazione di queste capacità con i prompt in linguaggio naturale ha reso possibile l’interazione intuitiva che oggi diamo per scontata.</p>
</section>
<section id="il-30-novembre-2022-il-giorno-in-cui-gli-llm-hanno-incontrato-il-mondo" class="level2">
<h2 class="anchored" data-anchor-id="il-30-novembre-2022-il-giorno-in-cui-gli-llm-hanno-incontrato-il-mondo">Il 30 Novembre 2022: Il Giorno in cui gli LLM hanno incontrato il Mondo</h2>
<p>Il 30 novembre 2022 OpenAI rilasciò ChatGPT al pubblico. GPT-3.5 era già un modello molto capace che sapeva conversare in modo naturale. Ma aveva altre tre caratteristiche vincenti: un’interfaccia conversazionale semplice e intuitiva, un eccellente allineamento che lo rendeva utile e sicuro, e soprattutto era gratuito e accessibile a chiunque.</p>
<p>L’impatto fu immediato e sconvolgente. Un milione di utenti in cinque giorni. Cento milioni in due mesi. Per la prima volta nella storia, gli LLM non erano più dominio esclusivo di ricercatori e grandi aziende tech. Era nelle mani di studenti, professionisti, creativi, curiosi di ogni tipo.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="immagini/llm_model_growth.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Crescita dei modelli LLM</figcaption>
</figure>
</div>
<p>Quello che seguì fu paragonabile all’esplosione del Cambriano, quel periodo 540 milioni di anni fa quando la vita sulla Terra si diversificò in modo esplosivo. Anthropic lanciò Claude, Meta rilasciò LLaMA in open source, Google rispose con Bard (poi Gemini), e decine di altre aziende e gruppi di ricerca si unirono alla corsa. Da allora non solo sono usciti centinaia di nuovi modelli, ma la ricerca scientifica sull’argomento è cresciuta esponenzialmente, con migliaia di paper pubblicati ogni mese che esplorano nuove frontiere e applicazioni.</p>
</section>
<section id="come-funziona-davvero-un-llm-la-verità-dietro-la-magia" class="level2">
<h2 class="anchored" data-anchor-id="come-funziona-davvero-un-llm-la-verità-dietro-la-magia">Come Funziona Davvero un LLM: La Verità Dietro la Magia</h2>
<p>Al cuore di ogni Large Language Model c’è un task apparentemente semplice: prevedere il token successivo più probabile. Quando scriviamo “Il cielo è”, il modello calcola le probabilità: “blu” potrebbe avere il 45% di probabilità, “nuvoloso” il 30%, “stellato” il 15%, e così via.</p>
<p>Ma da questo task basilare emergono capacità che sembrano umane. Il modello può tradurre lingue, scrivere codice, comporre poesie, risolvere problemi matematici, persino mostrare quello che sembra ragionamento logico. Sono tutte proprietà emergenti del task fondamentale di predizione.</p>
<p>In un recente studio pubblicato da Anthropic (<a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">Attribution Graphs in Biology</a>), i ricercatori hanno mostrato come i meccanismi interni siano più complessi della sola predizione. Attraverso tecniche di “circuit tracing”, hanno scoperto che nella complessità di un’architettura di deep learning che impara dal linguaggio umano emergono proprietà sorprendentemente simili al ragionamento. Anche se questo è più un ragionamento “alieno” - non è tutto e per tutto come quello degli umani - mostra capacità di pianificazione, di considerare multiple opzioni, e di costruire rappresentazioni astratte del mondo.</p>
<p>Questa natura probabilistica porta a fenomeni affascinanti. L’in-context learning permette al modello di “imparare” nuovi compiti semplicemente da esempi forniti nel prompt, senza modificare i suoi parametri. Il few-shot learning gli permette di generalizzare da pochissimi esempi.</p>
</section>
<section id="una-distinzione-cruciale" class="level2">
<h2 class="anchored" data-anchor-id="una-distinzione-cruciale">Una Distinzione Cruciale</h2>
<p>Quando usi ChatGPT o un sistema simile, è fondamentale capire che stai interagendo con tre livelli distinti. C’è il modello vero e proprio - l’AI che genera le risposte. C’è l’interfaccia - l’applicazione web o mobile attraverso cui interagisci, che può avere suoi filtri e logiche aggiuntive. E c’è il server dove fisicamente gira il modello, con tutte le implicazioni legali e di privacy che questo comporta.</p>
<p>Questa distinzione non è accademica. L’interfaccia può modificare o filtrare le risposte del modello. Il server determina chi ha accesso ai tuoi dati e sotto quale giurisdizione. Quando condividi informazioni sensibili con un chatbot, stai potenzialmente condividendole con tutti e tre questi livelli.</p>
<p>In questo video puoi vedere DeepSeek iniziare a generare una risposta, riprendersi a metà flusso, eliminare tutto e reindirizzare la conversazione: <a href="https://www.linkedin.com/posts/stuartmccall_this-is-wild-watch-deepseeks-ai-sensor-activity-7290021099356995586-ixLl/">Video LinkedIn</a></p>
<p>È importante anche distinguere tra allenamento e prompt. Se il modello ha visto una pagina di Wikipedia durante il training (l’ha vista quasi sicuramente, a meno che non sia nuova), questa conoscenza è diversa da quando metti la stessa informazione nel prompt. Gli LLM non hanno un database ma hanno imparato i pattern nel linguaggio e della conoscenza umana. Potresti pensare al prompt come la memoria a breve termine (memoria di lavoro) e i dati di training come quella a lungo termine.</p>
</section>
<section id="glossario-essenziale" class="level2">
<h2 class="anchored" data-anchor-id="glossario-essenziale">Glossario Essenziale</h2>
<dl>
<dt><strong>AI (Intelligenza Artificiale)</strong></dt>
<dd>
Qualunque sistema che risolve problemi che normalmente richiederebbero intelletto umano
</dd>
<dt><strong>Machine Learning</strong></dt>
<dd>
Approccio all’AI dove i sistemi imparano dai dati invece di essere programmati esplicitamente
</dd>
<dt><strong>Deep Learning</strong></dt>
<dd>
ML con reti neurali profonde (molti layer)
</dd>
<dt><strong>LLM (Large Language Model)</strong></dt>
<dd>
Modello di Deep Learning specializzato nel linguaggio, basato su architettura Transformer
</dd>
<dt><strong>Token</strong></dt>
<dd>
Unità base di testo processata dal modello
</dd>
<dt><strong>Embedding</strong></dt>
<dd>
Rappresentazione numerica complessa che cattura il “significato”
</dd>
<dt><strong>Parametri</strong></dt>
<dd>
Valori numerici nella rete neurale, regolati durante il training
</dd>
<dt><strong>Transformer</strong></dt>
<dd>
Architettura che usa il meccanismo di “attention”
</dd>
<dt><strong>Pre-training</strong></dt>
<dd>
Prima fase di addestramento su grandi quantità di testo
</dd>
<dt><strong>Fine-tuning</strong></dt>
<dd>
Specializzazione del modello per compiti specifici
</dd>
</dl>
</section>

</div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->

<div class="section-2" style="margin-top: 50px;">
<a href="/corso-llm/limiti-llm/">Prossimo modulo →</a>
</div>




</body></html>